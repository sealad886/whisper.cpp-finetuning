# Whisper Fine-Tuning Toolkit

This repository contains utilities for preparing manifests, fine-tuning OpenAI Whisper checkpoints with LoRA/DoRA adapters, merging adapters for deployment, and validating outputs. The primary entry point is `train_whisper_lora.py`, which can be driven entirely from JSON configuration files or CLI arguments.

## Quick Start

1. Install dependencies: `pip install -r requirements.txt`
2. Prepare CSV manifests with `build_manifest_from_names.py`
3. Launch training: `python train_whisper_lora.py --config configs/train_whisper_lora.json`

## Configuration Reference

Settings can be provided as CLI flags or in a JSON config. The script merges CLI overrides with the JSON payload.

### Data & Output

- `base_model`
  - **Purpose:** Selects the pretrained Whisper checkpoint (HF Hub ID) that acts as the initialization point.
  - **Training impact:** Determines model capacity, language coverage, tokenizer, and available acoustic priors. Larger checkpoints improve accuracy but demand more VRAM.
  - **How to set:** Provide a valid HF Hub identifier (`openai/whisper-medium.en`, `openai/whisper-large-v3`, etc.). Must be compatible with Whisper’s processor.
  - **Reasonable default:** `openai/whisper-medium.en` for balanced accuracy vs. cost; `openai/whisper-large-v3-turbo` for high-resource setups.

- `train_csv`, `val_csv`
  - **Purpose:** Define supervised data splits. Files must contain `path` (audio file), `text` (transcript), optional `language` column.
  - **Training impact:** Drives dataset loading, batching, and evaluation; mismatched sample rates or missing columns will raise errors.
  - **How to set:** Absolute/relative paths generated by `build_manifest_from_names.py`.
  - **Reasonable default:** Custom per-project; ensure validation split reflects target domain.

- `outdir`
  - **Purpose:** Storage root for Trainer outputs (checkpoints, optimizer states, logs) and saved processor artifacts.
  - **Training impact:** Trainer resumes from checkpoints in this folder; merging and conversion scripts expect its layout.
  - **How to set:** Writeable directory path. Script creates it; existing contents trigger resume/overwrite logic.
  - **Reasonable default:** `./whisper-<variant>-<experiment>`.

- `language`
  - **Purpose:** Sets Whisper’s generation config (language token) and default CSV language fallback.
  - **Training impact:** Ensures decoder emits correct language tokens; mismatched setting can degrade decoding.
  - **How to set:** Two-letter ISO-639-1 (e.g. `en`, `es`).
  - **Reasonable default:** Match dataset’s dominant language; `en` for English corpora.

### Training Schedule

- `seed`
  - **Purpose:** Seeds Python, NumPy, Torch, and HF utilities for reproducibility.
  - **Training impact:** Controls shuffling, weight init, dropout patterns; consistent seed enables comparable runs.
  - **How to set:** Integer; stick to one value per experiment family.
  - **Reasonable default:** `42` or `13`.

- `epochs`
  - **Purpose:** Number of full passes over the training set.
  - **Training impact:** Higher epochs risk overfitting but improve convergence on small datasets; interacts with early stopping if implemented externally.
  - **How to set:** Positive int; consider dataset size and adapter capacity.
  - **Reasonable default:** `3-5` for LoRA/DoRA fine-tuning.

- `lr`
  - **Purpose:** Optimizer learning rate.
  - **Training impact:** Too high destabilizes adapters; too low slows convergence. LoRA/DoRA often tolerate `5e-5` to `1e-4`.
  - **How to set:** Float; tune slightly downward for very small ranks or minimal data.
  - **Reasonable default:** `5e-5`.

- `warmup_ratio`
  - **Purpose:** Warmup percentage for linear scheduler.
  - **Training impact:** Smooths startup to prevent divergence, especially with higher LR.
  - **How to set:** Float between `0` and `0.2`; larger for noisy data.
  - **Reasonable default:** `0.08`.

- `per_dev_batch`
  - **Purpose:** Per-device batch size for train/eval dataloaders.
  - **Training impact:** Combined with gradient accumulation, determines effective batch size; constrained by GPU memory.
  - **How to set:** Integer; ensure audio segments fit in VRAM with feature extraction.
  - **Reasonable default:** `2` on 24GB GPUs; increase if memory allows.

- `grad_accum`
  - **Purpose:** Number of optimizer steps to delay updates while summing gradients.
  - **Training impact:** Increases effective batch size without extra memory cost; too large slows per-step updates.
  - **How to set:** Integer; `effective_batch = per_dev_batch * grad_accum * num_devices`.
  - **Reasonable default:** `4` to simulate batch size 8 on single GPU.

- `logging_steps`
  - **Purpose:** Interval for Trainer logging (loss, LR).
  - **Training impact:** Frequent logging increases console noise and minimal overhead.
  - **How to set:** Integer steps; adjust for dataset size.
  - **Reasonable default:** `50`.

- `save_steps`
  - **Purpose:** Checkpoint saving cadence.
  - **Training impact:** Controls disk usage vs. recovery resolution; with DoRA, checkpoints store magnitude vectors too.
  - **How to set:** Integer; smaller on volatile runs, larger to save disk.
  - **Reasonable default:** `500-1000`; example config uses `10` for rapid experimentation.

- `eval_steps`
  - **Purpose:** Validation frequency.
  - **Training impact:** Frequent eval provides feedback but lengthens training time.
  - **How to set:** Integer; align with save cadence if you want checkpoint-per-eval.
  - **Reasonable default:** `500`.

### Precision, Device & Encoder Control

- `fp16`
  - **Purpose:** Enable FP16 mixed-precision training via Accelerate.
  - **Training impact:** Reduces memory footprint, speeds compute on NVIDIA; can be unstable without loss scaling.
  - **How to set:** Boolean flag.
  - **Reasonable default:** `true` on Ampere+ GPUs, otherwise `false`.

- `bf16`
  - **Purpose:** Enable bfloat16 training when hardware supports it (A100, H100, newer TPUs).
  - **Training impact:** Offers wide exponent range; prefer over FP16 if available.
  - **How to set:** Boolean flag; mutually exclusive with `fp16`.
  - **Reasonable default:** `true` on supported hardware.

- `freeze_encoder`
  - **Purpose:** Locks encoder weights, training only decoder-side adapters.
  - **Training impact:** Cuts trainable parameter count; useful for rapid domain adaptation when acoustic features align with base model.
  - **How to set:** Boolean flag.
  - **Reasonable default:** `false` unless adapting purely textual output style.

### Adapter & DoRA Controls

- `lora_r`
  - **Purpose:** Rank (dimension) of low-rank adapter decomposition.
  - **Training impact:** Higher rank increases expressivity and memory cost; DoRA magnitudes amplify effect of lower ranks.
  - **How to set:** Positive int; 8–32 common.
  - **Reasonable default:** `16` for LoRA, `8`-`16` for DoRA trials.

- `lora_alpha`
  - **Purpose:** Scaling (alpha) applied to adapter output before adding to base layer.
  - **Training impact:** Controls adaptation strength; interacts with `lora_r` for effective update magnitude.
  - **How to set:** Integer; rule-of-thumb `alpha ≈ 2 * r`.
  - **Reasonable default:** `32` with `r=16`.

- `lora_dropout`
  - **Purpose:** Dropout probability on adapter inputs during training.
  - **Training impact:** Regularizes adapters, helpful on small datasets; set `0` for stable large-data runs.
  - **How to set:** Float in `[0,1)`.
  - **Reasonable default:** `0.05`.

- `lora_target_modules`
  - **Purpose:** List of module names (or regex) where adapters attach. Defaults cover Whisper attention projections plus MLP.
  - **Training impact:** Limiting to attention can reduce compute; including MLP improves lexical adaptation.
  - **How to set:** CLI list (`--lora-target-modules q_proj k_proj ...`) or JSON array.
  - **Reasonable default:** `['q_proj','k_proj','v_proj','o_proj','fc1','fc2']`.

- `modules_to_save`
  - **Purpose:** Additional modules kept trainable/saved with adapters (e.g., `embed_tokens`, classification heads).
  - **Training impact:** Ensures non-adapter tuning (like bias or output heads) persists when saving PEFT checkpoints.
  - **How to set:** List/array of module names.
  - **Reasonable default:** `null` unless customizing extra heads.

- `use_dora`
  - **Purpose:** Activates Weight-Decomposed LoRA. Adds magnitude vectors per layer to rescale base weights.
  - **Training impact:** Improves stability/accuracy at low ranks with modest overhead; magnitude vectors must be saved/merged.
  - **How to set:** Boolean flag.
  - **Reasonable default:** `false` for baseline; toggle `true` when benchmarking DoRA.

- `dora_ephemeral_offload`
  - **Purpose:** Convenience switch that injects `ephemeral_gpu_offload=True` into runtime config, speeding DoRA initialization when adapters live on CPU.
  - **Training impact:** Reduces DoRA init cost in CPU-offload setups; negligible effect if everything is on GPU.
  - **How to set:** Boolean flag.
  - **Reasonable default:** `true` when using CPU offload (`accelerate` zero3) or large models on limited VRAM.

- `lora_runtime_config`
  - **Purpose:** Raw JSON passed to `LoraRuntimeConfig` (currently mostly `ephemeral_gpu_offload`).
  - **Training impact:** Controls runtime behaviors like temporary GPU staging for DoRA magnitude computation.
  - **How to set:** JSON string/object, e.g. `{ "ephemeral_gpu_offload": true }`.
  - **Reasonable default:** `null` unless customizing DoRA runtime behavior.

- `lora_init`
  - **Purpose:** Chooses adapter weight initialization. Supports bool or named strategies (`"pissa"`, `"eva"`, `"orthogonal"`).
  - **Training impact:** Influences early convergence; PiSSA accelerates alignment, EVA redistributes rank, orthogonal keeps LoRA neutral.
  - **How to set:** CLI string or JSON string.
  - **Reasonable default:** `null` (PEFT default `True`), or `"pissa"` for quick ramp-up.

- `lora_rank_pattern`
  - **Purpose:** Dict mapping module regex/key to custom rank values.
  - **Training impact:** Allows per-layer capacity allocation—e.g., higher rank on decoder cross-attention for code-switching data.
  - **How to set:** JSON object: `{ "decoder.layers.0.encoder_attn.k_proj": 32 }`.
  - **Reasonable default:** `null` unless dataset suggests per-layer tuning.

- `lora_alpha_pattern`
  - **Purpose:** Dict mapping module regex/key to custom alpha scaling.
  - **Training impact:** Fine-grained control over update magnitude per layer; complements `lora_rank_pattern`.
  - **How to set:** JSON object: `{ "decoder.layers.*": 48 }`.
  - **Reasonable default:** `null`.

### Training Flow & Artifacts

- `merge_lora`
  - **Purpose:** After training, merge adapters into base weights and save under `<outdir>/merged`.
  - **Training impact:** Adds time post-training but produces ready-to-serve checkpoint without PEFT dependency; required before exporting to Whisper.cpp.
  - **How to set:** Boolean flag.
  - **Reasonable default:** `true` when deploying, `false` when iterating and keeping adapters separate.

## Tips

- Provide configs via `--config` to keep experiments reproducible.
- For DoRA runs, consider enabling `dora_ephemeral_offload` or custom `lora_runtime_config` to speed magnitude-vector init on CPU-heavy setups.
- After training, use `merge_lora` plus `merge_lora_and_convert.py` to prepare deployment artifacts (e.g., Whisper.cpp conversion).
